{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch ## version >= 1.8.2\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl ## version == 1.4.9\n",
    "\n",
    "import datasets ## version == 2.1.0\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel ## version == 4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "요즘은 아침에 일어나는 게 예전 같지가 않아요.\n",
    "→ 활력 저하 / 피로 / 노화 인식\n",
    "큰 병은 없는데, 그냥 하루가 너무 길어요.\n",
    "→ 무기력 / 공허감 (표면상 중립)\n",
    "애들한테 괜히 전화할까 봐 그냥 혼자 있어요.\n",
    "→ 고립감 / 부담 인식 / 관계 위축\n",
    "예전에는 이 정도는 혼자 다 했는데 말이죠.\n",
    "→ 상실감 / 자기효능감 저하\n",
    "밥은 먹었어요, 입맛이 없어서 조금만 먹었지만.\n",
    "→ 신체·정서 혼합 신호 (은근한 부정)\n",
    "사는 게 다 그렇죠 뭐.\n",
    "→ 체념 / 감정 억제 (중요한 시니어 특유 표현)\n",
    "괜히 내가 이렇게 살아야 하나 싶을 때가 있어요.\n",
    "→ 위험/경고 가능성 (자기 존재 가치 의문)\n",
    "손주 사진 보면 좋긴 한데, 또 금방 허전해져요.\n",
    "→ 긍정 + 공허 동시 존재 (혼합 감정)\n",
    "아프면 남한테 민폐일까 봐 병원 가는 것도 망설여져요.\n",
    "→ 불안 / 자기 억제 / 위험 신호\n",
    "누가 물어보면 괜찮다고는 하는데, 사실 잘 모르겠어요.\n",
    "→ 감정 비가시화 / 중립으로 오분류되기 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.txt 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import json\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=-1,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='sigmoid')\n",
    "\n",
    "sentences = [\n",
    "    \"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.\",\n",
    "    \"해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.\",\n",
    "    \"날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.\",\n",
    "    \"네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.\",\n",
    "    \"괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"\n",
    "]\n",
    "sentences1 = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  - {output['label']}: {output['score']:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"완료!\\n\")\n",
    "\n",
    "print(\"results.txt 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.txt 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=-1,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='sigmoid')\n",
    "\n",
    "sentences = [\n",
    "    \"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.\",\n",
    "    \"해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.\",\n",
    "    \"날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.\",\n",
    "    \"네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.\",\n",
    "    \"괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"\n",
    "]\n",
    "\n",
    "with open('results2.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0.4:\n",
    "                f.write(f\"  - {output['label']}: {output['score']:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"완료!\\n\")\n",
    "\n",
    "print(\"results.txt 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.txt 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import json\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=-1,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='none')\n",
    "\n",
    "sentences = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.\",\"해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.\",\"날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.\",\"네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.\",\"괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "sentences1 = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    # sentences 처리\n",
    "    f.write(\"[개별 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  - {output['label']}: {output['score']:.4f}\\n\")\n",
    "    \n",
    "    # sentences1 처리\n",
    "    f.write(\"\\n\\n[전체 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences1, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  - {output['label']}: {output['score']:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\n완료!\\n\")\n",
    "\n",
    "print(\"results.txt 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.txt 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import json\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=-1,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='sigmoid')\n",
    "\n",
    "sentences = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.\",\"해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.\",\"날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.\",\"네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.\",\"괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "sentences1 = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    # sentences 처리\n",
    "    f.write(\"[개별 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  {output['score']:.4f}\\n\")\n",
    "    \n",
    "    # sentences1 처리\n",
    "    f.write(\"\\n\\n[전체 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences1, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  {output['score']:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\n완료!\\n\")\n",
    "\n",
    "print(\"results.txt 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.txt 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import json\n",
    "#라벨만 출력\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=-1,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='sigmoid')\n",
    "\n",
    "sentences = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.\",\"해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.\",\"날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.\",\"네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.\",\"괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "sentences1 = [\"아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.\"]\n",
    "\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    # sentences 처리\n",
    "    f.write(\"[개별 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  {output['label']}\\n\")\n",
    "    \n",
    "    # sentences1 처리\n",
    "    f.write(\"\\n\\n[전체 문장 분석]\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for i, sentence in enumerate(sentences1, 1):\n",
    "        f.write(f\"\\n[{i}] 입력 문장: {sentence}\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        results = pipe(sentence)[0]-\n",
    "        for output in results:\n",
    "            if output[\"score\"] > 0:\n",
    "                f.write(f\"  {output['label']}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\n완료!\\n\")\n",
    "\n",
    "print(\"results.txt 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "train 분석 진행 중: 100%|██████████| 40000/40000 [28:32<00:00, 23.36it/s] \n",
      "val 분석 진행 중: 100%|██████████| 5000/5000 [03:46<00:00, 22.11it/s]\n",
      "test 분석 진행 중: 100%|██████████| 5000/5000 [03:47<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                                                텍스트      긍정  \\\n",
      "0      39087              내가 톰행크스를 좋아하긴 했나보다... 초기 영화 빼고는 다 봤네.  0.4789   \n",
      "1      30893  정말 상상을 초월하는 무개념 진상들 상대하다 우울증, 공항장애 걸리는 공무원 많아요...  0.0498   \n",
      "2      45278  새로운 세상과 조우한 자의 어린아이 같은 반응, 어쩌면 회복된 것은 눈이 아닌 순수...  0.4180   \n",
      "3      16398       미역은 원생생물계 산호초는 동물ㅇㅇ 아 미역이 바다의 새ㄱㅇㄱㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ  0.2164   \n",
      "4      13653                        네 맞습니다 플스는 역시 30프레임이 어울리죠 ㅎ  0.5436   \n",
      "...      ...                                                ...     ...   \n",
      "49995   6161  일단 내용만 보고놓자면 크게 신경쓸일은 아닌거같네요 한국룸메랑 같이 비교하지마세요 ...  0.2270   \n",
      "49996  21130  캐릭터가 프린트된 제품인줄 알았는데 스티커임 손으로 조물조물 빨았는데 캐릭터들이 쪼...  0.0560   \n",
      "49997   6788                          마지막에 마카롱으로 미키마우스만든거 넘귀엽다ㅋ  0.3128   \n",
      "49998  45946                    너무 멋진 친구를 두셨네요  작가님 함께 같이 살아갑시다  0.5985   \n",
      "49999   9159                          강려크하시니 오늘은 검색하지 않겟습니다. ㅋㅋ  0.2715   \n",
      "\n",
      "           중립   활력 저하      부정       Y   데이터셋  \n",
      "0      0.0622  0.0496  0.0602  0.6737  train  \n",
      "1      0.0273  0.4943  0.1517  1.0946  train  \n",
      "2      0.0467  0.0210  0.0362  0.5218  train  \n",
      "3      0.4479  0.0355  0.1357  0.5409  train  \n",
      "4      0.2885  0.0090  0.0177  0.5924  train  \n",
      "...       ...     ...     ...     ...    ...  \n",
      "49995  0.1764  0.2844  0.1037  0.8611   test  \n",
      "49996  0.0926  0.1529  0.2647  0.8146   test  \n",
      "49997  0.0247  0.0082  0.0100  0.3451   test  \n",
      "49998  0.0219  0.0138  0.0173  0.6538   test  \n",
      "49999  0.4063  0.1611  0.1655  0.8441   test  \n",
      "\n",
      "[50000 rows x 8 columns]\n",
      "\n",
      "총 50000개 행 처리 완료\n",
      "결과가 'emotion_classification_final.csv'로 저장되었습니다.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 136\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m결과가 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_classification_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m로 저장되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# 엑셀로 저장\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion_classification_final.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m결과가 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_classification_final.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m로 저장되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:2439\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2426\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2428\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2429\u001b[0m     df,\n\u001b[0;32m   2430\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2437\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2438\u001b[0m )\n\u001b[1;32m-> 2439\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2441\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  device=0,\n",
    "                                  return_all_scores=True,\n",
    "                                  function_to_apply='sigmoid')\n",
    "\n",
    "# 감정 분류 매핑\n",
    "emotion_mapping = {\n",
    "    # 긍정 (15개)\n",
    "    '환영/호의': '긍정',\n",
    "    '감동/감탄': '긍정',\n",
    "    '고마움': '긍정',\n",
    "    '존경': '긍정',\n",
    "    '기대감': '긍정',\n",
    "    '뿌듯함': '긍정',\n",
    "    '편안/쾌적': '긍정',\n",
    "    '신기함/관심': '긍정',\n",
    "    '아껴주는': '긍정',\n",
    "    '흐뭇함(귀여움/예쁨)': '긍정',\n",
    "    '행복': '긍정',\n",
    "    '안심/신뢰': '긍정',\n",
    "    '깨달음': '긍정',\n",
    "    '즐거움/신남': '긍정',\n",
    "    '기쁨': '긍정',\n",
    "    \n",
    "    # 중립 (1개)\n",
    "    '없음': '중립',\n",
    "    \n",
    "    # 부정 (20개)\n",
    "    '불평/불만': '부정',\n",
    "    '지긋지긋': '부정',\n",
    "    '화남/분노': '부정',\n",
    "    '의심/불신': '부정',\n",
    "    '부끄러움': '부정',\n",
    "    '공포/무서움': '부정',\n",
    "    '절망': '부정',\n",
    "    '한심함': '부정',\n",
    "    '역겨움/징그러움': '부정',\n",
    "    '짜증': '부정',\n",
    "    '어이없음': '부정',\n",
    "    '패배/자기혐오': '부정',\n",
    "    '증오/혐오': '부정',\n",
    "    '당황/난처': '부정',\n",
    "    '경악': '부정',\n",
    "    '놀람': '부정',\n",
    "    '불안/걱정': '부정',\n",
    "    '우쭐댐/무시함': '부정',\n",
    "    '비장함': '부정',\n",
    "    \n",
    "    # 활력 저하 (8개)\n",
    "    '슬픔': '활력 저하',\n",
    "    '안타까움/실망': '활력 저하',\n",
    "    '서러움': '활력 저하',\n",
    "    '힘듦/지침': '활력 저하',\n",
    "    '재미없음': '활력 저하',\n",
    "    '부담/안_내킴': '활력 저하',\n",
    "    '불쌍함/연민': '활력 저하',\n",
    "    '귀찮음': '활력 저하',\n",
    "    '죄책감': '부정',\n",
    "}\n",
    "\n",
    "# 각 분류별 감정 개수\n",
    "category_counts = {\n",
    "    '긍정': 15,\n",
    "    '중립': 1,\n",
    "    '부정': 20,\n",
    "    '활력 저하': 8,\n",
    "}\n",
    "\n",
    "def process_file(file_path, file_type):\n",
    "    \"\"\"파일을 읽고 감정 분석 수행\"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['id', 'text', 'labels'])\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{file_type} 분석 진행 중\"):\n",
    "        text = row['text']\n",
    "        outputs = pipe(text)[0]\n",
    "        \n",
    "        # 감정 분류별 스코어 합계\n",
    "        emotion_scores = {'긍정': 0, '중립': 0, '활력 저하': 0, '부정': 0}\n",
    "        \n",
    "        for output in outputs:\n",
    "            if output[\"score\"] > 0:\n",
    "                label = output['label']\n",
    "                score = output['score']\n",
    "                \n",
    "                # 라벨을 분류로 매핑\n",
    "                if label in emotion_mapping:\n",
    "                    category = emotion_mapping[label]\n",
    "                    emotion_scores[category] += score\n",
    "        \n",
    "        # 각 분류별 감정 개수로 나누기\n",
    "        positive = emotion_scores['긍정'] / category_counts['긍정']\n",
    "        neutral = emotion_scores['중립'] / category_counts['중립']\n",
    "        vitality = emotion_scores['활력 저하'] / category_counts['활력 저하']\n",
    "        negative = emotion_scores['부정'] / category_counts['부정']\n",
    "        \n",
    "        result = {\n",
    "            '긍정': round(positive, 4),\n",
    "            '중립': round(neutral, 4),\n",
    "            '활력 저하': round(vitality, 4),\n",
    "            '부정': round(negative, 4),\n",
    "            'Y': round(positive * 1 + neutral * 0 + vitality * 1.5 + negative * 2, 4),\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 3개 파일 처리\n",
    "train_df = process_file('train.tsv', 'train')\n",
    "val_df = process_file('val.tsv', 'val')\n",
    "test_df = process_file('test.tsv', 'test')\n",
    "\n",
    "# 모든 데이터 합치기\n",
    "final_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# 출력\n",
    "print(final_df)\n",
    "print(f\"\\n총 {len(final_df)}개 행 처리 완료\")\n",
    "\n",
    "# CSV로 저장\n",
    "final_df.to_csv('emotion_classification_final.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"결과가 'emotion_classification_final.csv'로 저장되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           긍정      중립   활력 저하      부정       Y\n",
      "0      0.4789  0.0622  0.0496  0.0602  0.6737\n",
      "1      0.0498  0.0273  0.4943  0.1517  1.0946\n",
      "2      0.4180  0.0467  0.0210  0.0362  0.5218\n",
      "3      0.2164  0.4479  0.0355  0.1357  0.5409\n",
      "4      0.5436  0.2885  0.0090  0.0177  0.5924\n",
      "...       ...     ...     ...     ...     ...\n",
      "49995  0.2270  0.1764  0.2844  0.1037  0.8611\n",
      "49996  0.0560  0.0926  0.1529  0.2647  0.8146\n",
      "49997  0.3128  0.0247  0.0082  0.0100  0.3451\n",
      "49998  0.5985  0.0219  0.0138  0.0173  0.6538\n",
      "49999  0.2715  0.4063  0.1611  0.1655  0.8441\n",
      "\n",
      "[50000 rows x 5 columns]\n",
      "결과가 'emotion_classification_final_cleaned.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 저장된 파일 읽기\n",
    "final_df = pd.read_csv('emotion_classification_final.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "result_df = final_df[['긍정', '중립', '활력 저하', '부정', 'Y']]\n",
    "\n",
    "# 출력\n",
    "print(result_df)\n",
    "\n",
    "# CSV로 저장\n",
    "result_df.to_csv('final_cleaned.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"결과가 'emotion_classification_final_cleaned.csv'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "model_name = \"searle-j/kote_for_easygoing_people\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0, # gpu number, -1 if cpu used\n",
    "        return_all_scores=True,\n",
    "        function_to_apply='sigmoid'\n",
    "    )\n",
    "\n",
    "for output in pipe(\"\"\"재미있어요! 재미는 확실히 있는데 뭐랄까... 너무 정신 없달까...ㅋㅋ\"\"\")[0]:\n",
    "    if output[\"score\"]>0.4:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1\n",
    "“오늘은 하루 어떻게 보내셨어요?”\n",
    "A1\n",
    "“아침에 일어나긴 했는데, 몸이 좀 무거워서 한참을 가만히 있었어요.”\n",
    "\n",
    "Q2\n",
    "“그래도 뭐라도 하시긴 하셨어요?”\n",
    "A2\n",
    "“해야 할 건 했죠. 밥도 먹고 설거지도 했는데, 예전처럼 손이 잘 안 가더라고요.”\n",
    "\n",
    "Q3\n",
    "“밖에 잠깐이라도 나가보실 생각은 안 드셨어요?”\n",
    "A3\n",
    "“날씨는 괜찮던데, 그냥 집에 있는 게 더 편해서요.”\n",
    "\n",
    "Q4\n",
    "“요즘은 집에 계시는 게 더 편하신가 봐요.”\n",
    "A4\n",
    "“네, 괜히 나갔다가 피곤해질까 봐요. 집에 있으면 조용하잖아요.”\n",
    "\n",
    "Q5\n",
    "“그렇게 하루를 보내면 기분은 어떠세요?”\n",
    "A5\n",
    "“괜찮다고는 하는데, 사실 하루가 그냥 흘러가는 느낌이에요.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "요즘은 아침에 일어나는 게 예전 같지가 않아요.\n",
    "→ 활력 저하 / 피로 / 노화 인식\n",
    "큰 병은 없는데, 그냥 하루가 너무 길어요.\n",
    "→ 무기력 / 공허감 (표면상 중립)\n",
    "애들한테 괜히 전화할까 봐 그냥 혼자 있어요.\n",
    "→ 고립감 / 부담 인식 / 관계 위축\n",
    "예전에는 이 정도는 혼자 다 했는데 말이죠.\n",
    "→ 상실감 / 자기효능감 저하\n",
    "밥은 먹었어요, 입맛이 없어서 조금만 먹었지만.\n",
    "→ 신체·정서 혼합 신호 (은근한 부정)\n",
    "사는 게 다 그렇죠 뭐.\n",
    "→ 체념 / 감정 억제 (중요한 시니어 특유 표현)\n",
    "괜히 내가 이렇게 살아야 하나 싶을 때가 있어요.\n",
    "→ 위험/경고 가능성 (자기 존재 가치 의문)\n",
    "손주 사진 보면 좋긴 한데, 또 금방 허전해져요.\n",
    "→ 긍정 + 공허 동시 존재 (혼합 감정)\n",
    "아프면 남한테 민폐일까 봐 병원 가는 것도 망설여져요.\n",
    "→ 불안 / 자기 억제 / 위험 신호\n",
    "누가 물어보면 괜찮다고는 하는데, 사실 잘 모르겠어요.\n",
    "→ 감정 비가시화 / 중립으로 오분류되기 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found kote.py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearle-j/kote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\datasets\\load.py:1492\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1488\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1489\u001b[0m )\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1492\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1493\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1494\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1495\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1496\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1497\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1498\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1499\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1500\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1501\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1502\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1503\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1505\u001b[0m )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\datasets\\load.py:1137\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[1;32m-> 1137\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\datasets\\load.py:1036\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1032\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1033\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1035\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\KOTE\\.venv\\lib\\site-packages\\datasets\\load.py:994\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[0;32m    988\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    989\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    992\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m    993\u001b[0m     )\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found kote.py"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"searle-j/kote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the shape.\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '32521',\n",
       " 'text': '구슬픈 봄날 저녁 무렵, 도시의 뒤섞여 있는 건축과 건축의 그림자를 찾아서 커다란 군중 속에 휩쓸려 가는 것은 얼마나 즐거운 일인가. <우울한 고양이_하기와라 사쿠타로>15',\n",
       " 'labels': [2, 4, 5, 13, 14, 15, 16, 27, 28, 38, 40, 42]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check one sample in the train set.\n",
    "\n",
    "dataset[\"train\"][25597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape ::: (40000, 44)\n",
      "test_labels shape :::: (5000, 44)\n",
      "val_labels shape ::::: (5000, 44)\n",
      "\n",
      "cool..!!\n"
     ]
    }
   ],
   "source": [
    "## convert the integer labels into multi-hot form (44-dimensional).\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(dataset[\"train\"][\"labels\"])\n",
    "test_labels = mlb.fit_transform(dataset[\"test\"][\"labels\"])\n",
    "val_labels = mlb.fit_transform(dataset[\"validation\"][\"labels\"])\n",
    "\n",
    "print(\"train_labels shape ::: {}\".format(train_labels.shape))\n",
    "print(\"test_labels shape :::: {}\".format(test_labels.shape))\n",
    "print(\"val_labels shape ::::: {}\".format(val_labels.shape))\n",
    "print(\"\\ncool..!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the texts, since we will use a custom datset not the huggingface dataset.\n",
    "\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "val_texts = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and the label names...\n",
    "\n",
    "LABELS = dataset[\"train\"].features[\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the pretrained tokenizer from huggingface.\n",
    "\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\" # <-- Thank you!\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let us mask and switch some tokens in the train set for a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_masking(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3]: # 0 ~ 3, [PAD], [UNK], [CLS], and [SEP], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = 4 # 4 is [MASK]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_switching(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3,4]: # 0 ~ 4, [PAD], [UNK], [CLS], [SEP], and [MASK], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = np.random.choice(np.arange(5,tokenizer.vocab_size), 1)[0]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_switch(encoding, prob=0.1):\n",
    "    encoding = token_masking(encoding, prob/2)\n",
    "    encoding = token_switching(encoding, prob/2)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maximum token lengths\n",
    "\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define our dataset...!\n",
    "\n",
    "class KOTEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length:int=MAX_LENGTH,\n",
    "                would_you_like_some_mask_and_switch:bool=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask = would_you_like_some_mask_and_switch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        if self.mask:\n",
    "            encoding = mask_and_switch(encoding, prob=0.1)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return dict(\n",
    "          input_ids=encoding[\"input_ids\"].flatten(),\n",
    "          attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "          labels=torch.FloatTensor(labels), ## must be a float tensor.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the datasets.\n",
    "\n",
    "train_dataset = KOTEDataset(train_texts, train_labels, tokenizer=tokenizer, would_you_like_some_mask_and_switch=True)\n",
    "test_dataset = KOTEDataset(test_texts, test_labels, tokenizer=tokenizer)\n",
    "val_dataset = KOTEDataset(val_texts, val_labels, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## download the pretrained electra model.\n",
    "\n",
    "electra = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"_name_or_path\": \"beomi/KcELECTRA-base\",\n",
       "  \"architectures\": [\n",
       "    \"ElectraForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"embedding_size\": 768,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"summary_activation\": \"gelu\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"first\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50135\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we will use the default arguments, except for the last gelu for classification.\n",
    "\n",
    "electra.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader with pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTEDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train_dataset, test_dataset, val_dataset, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 ## about 28 ~ 30 Gb memory required, if my memory serves me right.\n",
    "\n",
    "data_module = KOTEDataModule(\n",
    "  train_dataset,\n",
    "  test_dataset,\n",
    "  val_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTETagger(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, gamma_for_expLR=None):\n",
    "        super().__init__()\n",
    "        self.electra = electra\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 44) ## the label dimension == 44 <-- what an ominous number for asians though... <-- I didn't intend it!\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        \n",
    "        ## the loss\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state[:,0,:] ## [CLS] of the last hidden state\n",
    "        output = self.classifier(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss, output\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "\n",
    "        preds = outputs\n",
    "\n",
    "        y_true = list(labels.detach().cpu())\n",
    "        y_pred = list(preds.detach().cpu())\n",
    "\n",
    "        return {\"loss\": loss, \"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "    \n",
    "    def epoch_end(self, outputs, state=\"train\"):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for out in outputs:\n",
    "            y_true += out[\"y_true\"]\n",
    "            y_pred += out[\"y_pred\"]\n",
    "\n",
    "        self.log(state + \"_loss\", float(loss), on_epoch=True, prog_bar=True)\n",
    "        print(f\"[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}\")\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"train\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"val\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "          optimizer=optimizer,\n",
    "          lr_scheduler=dict(\n",
    "            scheduler=scheduler,\n",
    "            interval=\"step\"\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 12500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## determine the schedule for our optimizer\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
    "TOTAL_STEPS = steps_per_epoch * N_EPOCHS\n",
    "WARMUP_STEPS = TOTAL_STEPS // 5\n",
    "WARMUP_STEPS, TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model.\n",
    "\n",
    "model = KOTETagger(\n",
    "    n_warmup_steps=WARMUP_STEPS,\n",
    "    n_training_steps=TOTAL_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set a logger and some stuffs...\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "## the check point\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    ###dirpath=\"YOUR DIRECTORY PATH\",\n",
    "    filename=\"epoch{epoch}-val_loss{val_loss:.4f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "## for early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.00)\n",
    "\n",
    "## the logger\n",
    "logger = TensorBoardLogger(\"YOUR_FOLDER_NAME\", name=\"ONE_MORE_FOLDER_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## trainer\n",
    "\n",
    "N_EPOCHS = 15 ## redefine the number of the epochs, just to make sure there is no more room to improve.\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=[2], ## GPU number\n",
    "    progress_bar_refresh_rate=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type         | Params\n",
      "--------------------------------------------\n",
      "0 | electra    | ElectraModel | 123 M \n",
      "1 | classifier | Linear       | 33.8 K\n",
      "2 | criterion  | BCELoss      | 0     \n",
      "--------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.953   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319caaed06bf47c081d19b7e50cea5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 VAL] Loss: 0.7097086906433105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d82c6ca1da48309adaee6ddb0ce090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 VAL] Loss: 0.3745824098587036\n",
      "[Epoch 0 TRAIN] Loss: 0.4992274045944214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 VAL] Loss: 0.3168054521083832\n",
      "[Epoch 1 TRAIN] Loss: 0.34891679883003235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 VAL] Loss: 0.2815024256706238\n",
      "[Epoch 3 TRAIN] Loss: 0.2872730791568756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 VAL] Loss: 0.27795523405075073\n",
      "[Epoch 4 TRAIN] Loss: 0.27514782547950745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5 VAL] Loss: 0.27747949957847595\n",
      "[Epoch 5 TRAIN] Loss: 0.2677864134311676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6 VAL] Loss: 0.27661705017089844\n",
      "[Epoch 6 TRAIN] Loss: 0.261890709400177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7 VAL] Loss: 0.2767089307308197\n",
      "[Epoch 7 TRAIN] Loss: 0.25691649317741394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8 VAL] Loss: 0.2769763767719269\n",
      "[Epoch 8 TRAIN] Loss: 0.25327250361442566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 9 TRAIN] Loss: 0.2511317729949951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 10 TRAIN] Loss: 0.2502129375934601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 11 TRAIN] Loss: 0.25066524744033813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 12 TRAIN] Loss: 0.25033998489379883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 13 TRAIN] Loss: 0.2501673996448517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 14 TRAIN] Loss: 0.25027579069137573\n"
     ]
    }
   ],
   "source": [
    "## about 4 ~ 5 hours to reach the optimum...\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./YOUR_FOLDER_NAME/ONE_MORE_FOLDER_NAME/version_0/checkpoints/epoch9-val_loss0.2766.ckpt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "par_dir = './YOUR_FOLDER_NAME/ONE_MORE_FOLDER_NAME/version_0/checkpoints/'\n",
    "best_ckpt = sorted(glob(par_dir + '*.ckpt'))[-1]\n",
    "best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader = KOTETagger.load_from_checkpoint(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader.eval()\n",
    "gruesome_mind_reader.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환영/호의: 0.8828433752059937\n",
      "감동/감탄: 0.975392758846283\n",
      "고마움: 0.320080429315567\n",
      "존경: 0.7944819927215576\n",
      "기대감: 0.722480297088623\n",
      "뿌듯함: 0.4538567066192627\n",
      "신기함/관심: 0.8110296726226807\n",
      "아껴주는: 0.8170523643493652\n",
      "즐거움/신남: 0.7045897841453552\n",
      "흐뭇함(귀여움/예쁨): 0.6821712255477905\n",
      "놀람: 0.4573516249656677\n",
      "행복: 0.6389814019203186\n",
      "기쁨: 0.7407119870185852\n",
      "안심/신뢰: 0.5029057860374451\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.3\n",
    "\n",
    "sample_text = \"고니요? 제가 아는 타짜 중에 최고였어요...\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_text,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "_, predictions = gruesome_mind_reader(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "predictions = predictions.flatten().numpy()\n",
    "for l,p in zip(LABELS, predictions):\n",
    "    if p < THRESHOLD:\n",
    "        continue\n",
    "    print(f\"{l}: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff5ee0a5f6343d4b1c8f5e171df715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\") ## set the GPU number!\n",
    "gruesome_mind_reader = gruesome_mind_reader.to(DEVICE)\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(test_dataset):\n",
    "    _, pred = gruesome_mind_reader(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(DEVICE),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(DEVICE)\n",
    "        )\n",
    "    predictions.append(pred.flatten())\n",
    "    labels.append(item[\"labels\"].round().int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8642)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "THRESHOLD = 0.3\n",
    "accuracy(predictions, labels, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC per tag\n",
      "0:: 불평/불만: 0.9365819692611694\n",
      "1:: 환영/호의: 0.8934606909751892\n",
      "2:: 감동/감탄: 0.9294263124465942\n",
      "3:: 지긋지긋: 0.8327970504760742\n",
      "4:: 고마움: 0.9178640842437744\n",
      "5:: 슬픔: 0.9033969044685364\n",
      "6:: 화남/분노: 0.9369299411773682\n",
      "7:: 존경: 0.9155579209327698\n",
      "8:: 기대감: 0.8818700909614563\n",
      "9:: 우쭐댐/무시함: 0.8310950994491577\n",
      "10:: 안타까움/실망: 0.8795443177223206\n",
      "11:: 비장함: 0.860582172870636\n",
      "12:: 의심/불신: 0.8715171217918396\n",
      "13:: 뿌듯함: 0.8662175536155701\n",
      "14:: 편안/쾌적: 0.8772338628768921\n",
      "15:: 신기함/관심: 0.8687111139297485\n",
      "16:: 아껴주는: 0.8906732797622681\n",
      "17:: 부끄러움: 0.7513707876205444\n",
      "18:: 공포/무서움: 0.8868585228919983\n",
      "19:: 절망: 0.8480616807937622\n",
      "20:: 한심함: 0.8787931203842163\n",
      "21:: 역겨움/징그러움: 0.8980967402458191\n",
      "22:: 짜증: 0.9233975410461426\n",
      "23:: 어이없음: 0.8891122341156006\n",
      "24:: 없음: 0.8734162449836731\n",
      "25:: 패배/자기혐오: 0.8482953310012817\n",
      "26:: 귀찮음: 0.8192627429962158\n",
      "27:: 힘듦/지침: 0.8519691824913025\n",
      "28:: 즐거움/신남: 0.933516263961792\n",
      "29:: 깨달음: 0.8232670426368713\n",
      "30:: 죄책감: 0.8641752600669861\n",
      "31:: 증오/혐오: 0.9314982891082764\n",
      "32:: 흐뭇함(귀여움/예쁨): 0.9215825796127319\n",
      "33:: 당황/난처: 0.8416628837585449\n",
      "34:: 경악: 0.8427996635437012\n",
      "35:: 부담/안_내킴: 0.7902610898017883\n",
      "36:: 서러움: 0.8463318347930908\n",
      "37:: 재미없음: 0.8799682855606079\n",
      "38:: 불쌍함/연민: 0.8644571304321289\n",
      "39:: 놀람: 0.8527653217315674\n",
      "40:: 행복: 0.9214766621589661\n",
      "41:: 불안/걱정: 0.8555545806884766\n",
      "42:: 기쁨: 0.9326159954071045\n",
      "43:: 안심/신뢰: 0.8924616575241089\n",
      "\n",
      "MACRO_AVG :: 0.8762837052345276\n"
     ]
    }
   ],
   "source": [
    "## we should check the roc scores, since KOTE is imbalanced..!\n",
    "\n",
    "macro_auroc = []\n",
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(LABELS):\n",
    "    try:\n",
    "        tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n",
    "        macro_auroc.append(tag_auroc)\n",
    "        print(f\"{i}:: {str(name)}: {tag_auroc}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print()\n",
    "print(\"MACRO_AVG :: {}\".format(np.array(macro_auroc).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       불평/불만       0.79      0.89      0.84      2113\n",
      "       환영/호의       0.55      0.82      0.66      1109\n",
      "       감동/감탄       0.67      0.86      0.76      1323\n",
      "        지긋지긋       0.47      0.57      0.51       816\n",
      "         고마움       0.56      0.71      0.62       637\n",
      "          슬픔       0.59      0.64      0.61       545\n",
      "       화남/분노       0.74      0.86      0.79      1538\n",
      "          존경       0.51      0.69      0.59       460\n",
      "         기대감       0.58      0.81      0.68      1359\n",
      "     우쭐댐/무시함       0.44      0.50      0.47       743\n",
      "     안타까움/실망       0.69      0.88      0.77      2185\n",
      "         비장함       0.47      0.46      0.46       416\n",
      "       의심/불신       0.62      0.77      0.69      1539\n",
      "         뿌듯함       0.43      0.56      0.49       602\n",
      "       편안/쾌적       0.45      0.51      0.48       458\n",
      "      신기함/관심       0.57      0.77      0.66      1346\n",
      "        아껴주는       0.56      0.70      0.63       897\n",
      "        부끄러움       0.33      0.07      0.11       306\n",
      "      공포/무서움       0.42      0.29      0.35       164\n",
      "          절망       0.48      0.44      0.46       472\n",
      "         한심함       0.64      0.80      0.71      1519\n",
      "    역겨움/징그러움       0.50      0.61      0.55       516\n",
      "          짜증       0.76      0.86      0.81      1909\n",
      "        어이없음       0.71      0.87      0.78      2055\n",
      "          없음       0.53      0.59      0.56       725\n",
      "     패배/자기혐오       0.38      0.25      0.30       208\n",
      "         귀찮음       0.41      0.22      0.29       290\n",
      "       힘듦/지침       0.50      0.47      0.48       473\n",
      "      즐거움/신남       0.68      0.85      0.76      1321\n",
      "         깨달음       0.49      0.62      0.55      1030\n",
      "         죄책감       0.00      0.00      0.00        84\n",
      "       증오/혐오       0.66      0.76      0.71       984\n",
      " 흐뭇함(귀여움/예쁨)       0.58      0.64      0.61       524\n",
      "       당황/난처       0.55      0.71      0.62      1319\n",
      "          경악       0.44      0.50      0.47       704\n",
      "     부담/안_내킴       0.41      0.32      0.36       606\n",
      "         서러움       0.41      0.33      0.37       263\n",
      "        재미없음       0.66      0.52      0.59       470\n",
      "      불쌍함/연민       0.53      0.59      0.56       685\n",
      "          놀람       0.55      0.63      0.59       922\n",
      "          행복       0.56      0.80      0.66       906\n",
      "       불안/걱정       0.53      0.64      0.58       960\n",
      "          기쁨       0.64      0.85      0.73      1205\n",
      "       안심/신뢰       0.54      0.73      0.62       945\n",
      "\n",
      "   micro avg       0.60      0.72      0.66     39651\n",
      "   macro avg       0.54      0.61      0.56     39651\n",
      "weighted avg       0.60      0.72      0.65     39651\n",
      " samples avg       0.61      0.75      0.65     39651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "upper, lower = 1, 0\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "print(classification_report(\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  target_names=LABELS,\n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed # ::: 5000\n",
      "MCC  :::::::::  0.588452811767147\n"
     ]
    }
   ],
   "source": [
    "## computation of some vector is impossible if it is a zero vector with zero variance. --> just turn off error signs\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef as MCC\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"error\")\n",
    "    \n",
    "    totalCorr = 0\n",
    "    totalLen = 0\n",
    "    for i in range(10_000):\n",
    "        try:\n",
    "            totalCorr += MCC(y_pred[i], y_true[i])\n",
    "            totalLen += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('computed # ::: {}'.format(totalLen))\n",
    "print('MCC  :::::::::  {}'.format(totalCorr/totalLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
